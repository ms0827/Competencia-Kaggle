# =============================================================================
registerDoParallel(cores = parallel::detectCores() - 1)
grid_fit <- tune_grid(
object       = modelo_tree,
# El objeto recipe no tiene que estar entrenado
preprocessor = transformer,
# Las resamples se tienen que haber creado con los datos sin
# prerocesar
resamples    = cv_folds,
metrics      = metric_set(rmse, mae),
control      = control_grid(save_pred = TRUE),
# Número de combinaciones generadas automáticamente
grid         = 70
)
stopImplicitCluster()
# grid_fit %>% unnest(.metrics) %>% head()
grid_fit %>% collect_metrics(summarize = TRUE) %>% head()
grid_fit %>% show_best(metric = "rmse", n = 5)
grid_fit %>%
collect_metrics(summarize = TRUE) %>%
filter(.metric == "rmse") %>%
select(-c(.estimator, n)) %>%
pivot_longer(
cols = c(tree_depth, min_n),
values_to = "value",
names_to = "parameter"
) %>%
ggplot(aes(x = value, y = mean, color = parameter)) +
geom_point() +
geom_line() +
labs(title = "Evolución del error en función de los hiperparámetros") +
facet_wrap(facets = vars(parameter), nrow = 2, scales = "free") +
theme_bw() +
theme(legend.position = "none")
grid_fit %>%
collect_metrics(summarize = TRUE) %>%
filter(.metric == "rmse") %>%
select(-c(.estimator, n)) %>%
ggplot(aes(x = tree_depth, y = min_n, color = mean, size = mean)) +
geom_point() +
scale_color_viridis_c() +
labs(title = "Evolución del error en función de los hiperparámetros") +
theme_bw()
# DEFINICIÓN DEL MODELO Y DE LOS HIPERPARÁMETROS A OPTIMIZAR
# =============================================================================
modelo_tree <- decision_tree(
mode       = "regression",
tree_depth = tune(),
min_n      = tune()
) %>%
set_engine(engine = "rpart")
# DEFINICIÓN DE LA ESTRATEGIA DE VALIDACIÓN Y CREACIÓN DE PARTICIONES
# =============================================================================
set.seed(1234)
cv_folds <- vfold_cv(
data    = datos_train,
v       = 5,
strata  = median_house_value
)
# GRID DE HIPERPARÁMETROS
# =============================================================================
set.seed(1234)
hiperpar_grid <- grid_random(
# Rango de búsqueda para cada hiperparámetro
tree_depth(range = c(1, 10), trans = NULL),
min_n(range      = c(2, 100), trans = NULL),
# Número combinaciones aleatorias probadas
size = 50
)
# EJECUCIÓN DE LA OPTIMIZACIÓN DE HIPERPARÁMETROS
# =============================================================================
registerDoParallel(cores = parallel::detectCores() - 1)
grid_fit <- tune_grid(
object       = modelo_tree,
# El objeto recipe no tiene que estar entrenado
preprocessor = transformer,
# Las resamples se tienen que haber creado con los datos sin
# prerocesar
resamples    = cv_folds,
metrics      = metric_set(rmse, mae),
control      = control_resamples(save_pred = TRUE),
# Hiperparámetros
grid         = hiperpar_grid
)
stopImplicitCluster()
grid_fit %>% show_best(metric = "rmse", n = 5)
# Selección de los mejores hiperparámetros encontrados
mejores_hiperpar <- select_best(grid_fit, metric = "rmse")
modelo_tree_final <- finalize_model(x = modelo_tree, parameters = mejores_hiperpar)
modelo_tree_final
modelo_tree_final_fit  <- modelo_tree_final %>%
fit(
formula = median_house_value ~ .,
data    = datos_train_prep
#data   = bake(transformer_fit, datos_train)
)
datos_test_prep  <- bake(transformer_fit, new_data = test)
table(datos$ocean_proximity)
predicciones <- modelo_tree_final_fit %>%
predict(
new_data = datos_test_prep,
#new_data = bake(transformer_fit, datos_test),
type = "numeric"
)
predicciones %>% head()
predictions<-predicciones
predictions<-data.frame(predictions)
predictions$id <-test$id
colnames(predictions)[colnames(predictions) == '.pred'] <- 'median_house_value'
predictions<-predictions %>%
dplyr::select(id,median_house_value)
#predictions
# predictions$median_house_value<-(predictions$median_house_value)^2
predictions
write.csv(x = predictions, file = "test_predict.csv", row.names = FALSE)
# Cargar librerias
library(tidymodels)
library(tidyverse)
library(skimr)
library(DataExplorer)
library(ggpubr)
library(univariateML)
library(GGally)
library(doParallel)
library(VIM)
library(randomForest)
# Cargar librerias
library(tidymodels)
library(tidyverse)
library(skimr)
library(DataExplorer)
library(ggpubr)
library(univariateML)
library(GGally)
library(doParallel)
library(VIM)
library(randomForest)
modelo_tree <- randomForest(mode = "regression") %>%
set_engine(engine = "rpart")
rm(list = ls())
# Cargando librerias
# ==============================================================================
library(skimr)
library(caret)
library(lattice)
library(ggplot2)
library(VIM)
library(reshape2)
library(randomForest)
library(caret)
library(doSNOW)
library(doParallel)
library(dplyr)
library(tidyverse)
# Carga de los datos de entrenamiento y prueba
# ==============================================================================
train<- read.csv("train.csv", sep=",")
test<- read.csv("test.csv", sep = ",")
# Tranformaciones de variables categoricas a numericas
# ==============================================================================
subData<- train %>%
dplyr::select(ocean_proximity)
OHE<-dummyVars("~.", data=train)
train<-data.frame(predict(OHE, newdata=train))
subData2<- test %>%
dplyr::select(ocean_proximity)
OHE<-dummyVars("~.", data=test)
test<-data.frame(predict(OHE, newdata=test))
# Imputacion de datos de forma aleatoria
# ==============================================================================
rand.imput <-function(x){
missing <- (is.na(x)) #vector booleano
n.missing <- sum(missing)#Numero de NA’s
x.obs <- x[!missing]#Datos no NA
imputed <- x
imputed[missing] <- sample(x.obs,n.missing,replace = T)
#Se extrae una muestra aleatoria conocida y se remplazan estos en los NA
return(imputed)}
#train$total_bedrooms <- rand.imput(train$total_bedrooms)
#test$total_bedrooms <- rand.imput(test$total_bedrooms)
# Imputacion de datos faltantes usando knn
# ==============================================================================
kn=5
train<- kNN(train,variable=c("total_bedrooms"),k=kn)
train<- subset( train, select = -c(total_bedrooms_imp) )
test<- kNN(test,variable=c("total_bedrooms"),k=kn)
test<- subset(test, select = -c(total_bedrooms_imp) )
# Random Forest CV entrenamiento del modelo
# ==============================================================================
set.seed(1234)
train_3<-train
train_3
set.seed(431)
tuneGrid <- data.frame(
.mtry = 8,
.splitrule = "maxstat",
.min.node.size = 8
)
modRF <- caret::train(median_house_value ~ ., data=train_3, method="ranger", trControl=trainControl(method="cv",number=3),preProcess = c("nzv","scale"),
tuneGrid = tuneGrid,num.trees = 1500,importance = 'permutation',tuneLength = 15)
# Visualizacion de las caracteristicas del modelo
# ==============================================================================
modRF
varImp(modRF)
train
rf.predict <-predict(modRF, test)
# Desnormalizar los datos
# ==============================================================================
#rf.predict <- rf.predict$net.result*(max(datos$median_house_value)-min(datos$median_house_value))+min(datos$median_house_value)
predictions<-rf.predict
predictions<-data.frame(predictions)
predictions$id <-test$id
colnames(predictions)[colnames(predictions) == 'predictions'] <- 'median_house_value'
predictions<-predictions %>%
dplyr::select(id,median_house_value)
predictions
write.csv(x = predictions, file = "test_predict.csv", row.names = FALSE)
# Carga de los datos de entrenamiento y prueba
# ==============================================================================
train<- read.csv("train.csv", sep=",")
test<- read.csv("test.csv", sep = ",")
# Tranformaciones de variables categoricas a numericas
# ==============================================================================
subData<- train %>%
dplyr::select(ocean_proximity)
OHE<-dummyVars("~.", data=train)
train<-data.frame(predict(OHE, newdata=train))
subData2<- test %>%
dplyr::select(ocean_proximity)
OHE<-dummyVars("~.", data=test)
test<-data.frame(predict(OHE, newdata=test))
# Imputacion de datos de forma aleatoria
# ==============================================================================
rand.imput <-function(x){
missing <- (is.na(x)) #vector booleano
n.missing <- sum(missing)#Numero de NA’s
x.obs <- x[!missing]#Datos no NA
imputed <- x
imputed[missing] <- sample(x.obs,n.missing,replace = T)
#Se extrae una muestra aleatoria conocida y se remplazan estos en los NA
return(imputed)}
#train$total_bedrooms <- rand.imput(train$total_bedrooms)
#test$total_bedrooms <- rand.imput(test$total_bedrooms)
# Imputacion de datos faltantes usando knn
# ==============================================================================
kn=5
train<- kNN(train,variable=c("total_bedrooms"),k=kn)
train<- subset( train, select = -c(total_bedrooms_imp) )
test<- kNN(test,variable=c("total_bedrooms"),k=kn)
test<- subset(test, select = -c(total_bedrooms_imp) )
# Random Forest CV entrenamiento del modelo
# ==============================================================================
set.seed(1234)
train_3<-train
train_3
set.seed(431)
tuneGrid <- data.frame(
.mtry = 8,
.splitrule = "maxstat",
.min.node.size = 8
)
modRF <- caret::train(median_house_value ~ ., data=train_3, method="ranger", trControl=trainControl(method="cv",number=3),preProcess = c("center","scale"),
tuneGrid = tuneGrid,num.trees = 500,importance = 'permutation',tuneLength = 15)
# Visualizacion de las caracteristicas del modelo
# ==============================================================================
modRF
# Carga de los datos de entrenamiento y prueba
# ==============================================================================
train<- read.csv("train.csv", sep=",")
test<- read.csv("test.csv", sep = ",")
# Tranformaciones de variables categoricas a numericas
# ==============================================================================
subData<- train %>%
dplyr::select(ocean_proximity)
OHE<-dummyVars("~.", data=train)
train<-data.frame(predict(OHE, newdata=train))
subData2<- test %>%
dplyr::select(ocean_proximity)
OHE<-dummyVars("~.", data=test)
test<-data.frame(predict(OHE, newdata=test))
# Imputacion de datos de forma aleatoria
# ==============================================================================
rand.imput <-function(x){
missing <- (is.na(x)) #vector booleano
n.missing <- sum(missing)#Numero de NA’s
x.obs <- x[!missing]#Datos no NA
imputed <- x
imputed[missing] <- sample(x.obs,n.missing,replace = T)
#Se extrae una muestra aleatoria conocida y se remplazan estos en los NA
return(imputed)}
#train$total_bedrooms <- rand.imput(train$total_bedrooms)
#test$total_bedrooms <- rand.imput(test$total_bedrooms)
# Imputacion de datos faltantes usando knn
# ==============================================================================
kn=5
train<- kNN(train,variable=c("total_bedrooms"),k=kn)
train<- subset( train, select = -c(total_bedrooms_imp) )
test<- kNN(test,variable=c("total_bedrooms"),k=kn)
test<- subset(test, select = -c(total_bedrooms_imp) )
# Random Forest CV entrenamiento del modelo
# ==============================================================================
set.seed(1234)
train_3<-train
train_3
set.seed(431)
tuneGrid <- data.frame(
.mtry = 8,
.splitrule = "maxstat",
.min.node.size = 8
)
modRF <- caret::train(median_house_value ~ ., data=train_3, method="ranger", trControl=trainControl(method="cv",number=3),preProcess = c("center","knnimpute"),
tuneGrid = tuneGrid,num.trees = 500,importance = 'permutation',tuneLength = 15)
# Carga de los datos de entrenamiento y prueba
# ==============================================================================
train<- read.csv("train.csv", sep=",")
test<- read.csv("test.csv", sep = ",")
# Tranformaciones de variables categoricas a numericas
# ==============================================================================
subData<- train %>%
dplyr::select(ocean_proximity)
OHE<-dummyVars("~.", data=train)
train<-data.frame(predict(OHE, newdata=train))
subData2<- test %>%
dplyr::select(ocean_proximity)
OHE<-dummyVars("~.", data=test)
test<-data.frame(predict(OHE, newdata=test))
# Imputacion de datos de forma aleatoria
# ==============================================================================
rand.imput <-function(x){
missing <- (is.na(x)) #vector booleano
n.missing <- sum(missing)#Numero de NA’s
x.obs <- x[!missing]#Datos no NA
imputed <- x
imputed[missing] <- sample(x.obs,n.missing,replace = T)
#Se extrae una muestra aleatoria conocida y se remplazan estos en los NA
return(imputed)}
#train$total_bedrooms <- rand.imput(train$total_bedrooms)
#test$total_bedrooms <- rand.imput(test$total_bedrooms)
# Imputacion de datos faltantes usando knn
# ==============================================================================
kn=5
train<- kNN(train,variable=c("total_bedrooms"),k=kn)
train<- subset( train, select = -c(total_bedrooms_imp) )
test<- kNN(test,variable=c("total_bedrooms"),k=kn)
test<- subset(test, select = -c(total_bedrooms_imp) )
# Random Forest CV entrenamiento del modelo
# ==============================================================================
set.seed(1234)
train_3<-train
train_3
set.seed(431)
tuneGrid <- data.frame(
.mtry = 8,
.splitrule = "maxstat",
.min.node.size = 8
)
modRF <- caret::train(median_house_value ~ ., data=train_3, method="ranger", trControl=trainControl(method="cv",number=3),preProcess = c("center","knnImpute"),
tuneGrid = tuneGrid,num.trees = 500,importance = 'permutation',tuneLength = 15)
# Visualizacion de las caracteristicas del modelo
# ==============================================================================
modRF
# Carga de los datos de entrenamiento y prueba
# ==============================================================================
train<- read.csv("train.csv", sep=",")
test<- read.csv("test.csv", sep = ",")
# Tranformaciones de variables categoricas a numericas
# ==============================================================================
subData<- train %>%
dplyr::select(ocean_proximity)
OHE<-dummyVars("~.", data=train)
train<-data.frame(predict(OHE, newdata=train))
subData2<- test %>%
dplyr::select(ocean_proximity)
OHE<-dummyVars("~.", data=test)
test<-data.frame(predict(OHE, newdata=test))
# Imputacion de datos de forma aleatoria
# ==============================================================================
rand.imput <-function(x){
missing <- (is.na(x)) #vector booleano
n.missing <- sum(missing)#Numero de NA’s
x.obs <- x[!missing]#Datos no NA
imputed <- x
imputed[missing] <- sample(x.obs,n.missing,replace = T)
#Se extrae una muestra aleatoria conocida y se remplazan estos en los NA
return(imputed)}
#train$total_bedrooms <- rand.imput(train$total_bedrooms)
#test$total_bedrooms <- rand.imput(test$total_bedrooms)
# Imputacion de datos faltantes usando knn
# ==============================================================================
kn=5
train<- kNN(train,variable=c("total_bedrooms"),k=kn)
train<- subset( train, select = -c(total_bedrooms_imp) )
test<- kNN(test,variable=c("total_bedrooms"),k=kn)
test<- subset(test, select = -c(total_bedrooms_imp) )
# Random Forest CV entrenamiento del modelo
# ==============================================================================
set.seed(1234)
train_3<-train
train_3
set.seed(431)
tuneGrid <- data.frame(
.mtry = 8,
.splitrule = "maxstat",
.min.node.size = 8
)
modRF <- caret::train(median_house_value ~ ., data=train_3, method="ranger", trControl=trainControl(method="cv",number=3),preProcess = c("scale","knnImpute"),
tuneGrid = tuneGrid,num.trees = 500,importance = 'permutation',tuneLength = 15)
# Visualizacion de las caracteristicas del modelo
# ==============================================================================
modRF
# Carga de los datos de entrenamiento y prueba
# ==============================================================================
train<- read.csv("train.csv", sep=",")
test<- read.csv("test.csv", sep = ",")
# Tranformaciones de variables categoricas a numericas
# ==============================================================================
subData<- train %>%
dplyr::select(ocean_proximity)
OHE<-dummyVars("~.", data=train)
train<-data.frame(predict(OHE, newdata=train))
subData2<- test %>%
dplyr::select(ocean_proximity)
OHE<-dummyVars("~.", data=test)
test<-data.frame(predict(OHE, newdata=test))
# Imputacion de datos de forma aleatoria
# ==============================================================================
rand.imput <-function(x){
missing <- (is.na(x)) #vector booleano
n.missing <- sum(missing)#Numero de NA’s
x.obs <- x[!missing]#Datos no NA
imputed <- x
imputed[missing] <- sample(x.obs,n.missing,replace = T)
#Se extrae una muestra aleatoria conocida y se remplazan estos en los NA
return(imputed)}
#train$total_bedrooms <- rand.imput(train$total_bedrooms)
#test$total_bedrooms <- rand.imput(test$total_bedrooms)
# Imputacion de datos faltantes usando knn
# ==============================================================================
kn=5
train<- kNN(train,variable=c("total_bedrooms"),k=kn)
train<- subset( train, select = -c(total_bedrooms_imp) )
test<- kNN(test,variable=c("total_bedrooms"),k=kn)
test<- subset(test, select = -c(total_bedrooms_imp) )
# Random Forest CV entrenamiento del modelo
# ==============================================================================
set.seed(1234)
train_3<-train
train_3
set.seed(431)
tuneGrid <- data.frame(
.mtry = 6,
.splitrule = "maxstat",
.min.node.size = 8
)
modRF <- caret::train(median_house_value ~ ., data=train_3, method="ranger", trControl=trainControl(method="cv",number=3),preProcess = c("scale","knnImpute"),
tuneGrid = tuneGrid,num.trees = 500,importance = 'permutation',tuneLength = 15)
# Visualizacion de las caracteristicas del modelo
# ==============================================================================
modRF
# Carga de los datos de entrenamiento y prueba
# ==============================================================================
train<- read.csv("train.csv", sep=",")
test<- read.csv("test.csv", sep = ",")
# Tranformaciones de variables categoricas a numericas
# ==============================================================================
subData<- train %>%
dplyr::select(ocean_proximity)
OHE<-dummyVars("~.", data=train)
train<-data.frame(predict(OHE, newdata=train))
subData2<- test %>%
dplyr::select(ocean_proximity)
OHE<-dummyVars("~.", data=test)
test<-data.frame(predict(OHE, newdata=test))
# Imputacion de datos de forma aleatoria
# ==============================================================================
rand.imput <-function(x){
missing <- (is.na(x)) #vector booleano
n.missing <- sum(missing)#Numero de NA’s
x.obs <- x[!missing]#Datos no NA
imputed <- x
imputed[missing] <- sample(x.obs,n.missing,replace = T)
#Se extrae una muestra aleatoria conocida y se remplazan estos en los NA
return(imputed)}
#train$total_bedrooms <- rand.imput(train$total_bedrooms)
#test$total_bedrooms <- rand.imput(test$total_bedrooms)
# Imputacion de datos faltantes usando knn
# ==============================================================================
kn=5
train<- kNN(train,variable=c("total_bedrooms"),k=kn)
train<- subset( train, select = -c(total_bedrooms_imp) )
test<- kNN(test,variable=c("total_bedrooms"),k=kn)
test<- subset(test, select = -c(total_bedrooms_imp) )
# Random Forest CV entrenamiento del modelo
# ==============================================================================
set.seed(1234)
train_3<-train
train_3
set.seed(431)
tuneGrid <- data.frame(
.mtry =9,
.splitrule = "maxstat",
.min.node.size = 8
)
modRF <- caret::train(median_house_value ~ ., data=train_3, method="ranger", trControl=trainControl(method="cv",number=3),preProcess = c("scale","knnImpute"),
tuneGrid = tuneGrid,num.trees = 500,importance = 'permutation',tuneLength = 15)
# Visualizacion de las caracteristicas del modelo
# ==============================================================================
modRF
modelo_rf <- randomForest(mode = "regression")
modelo_rf <- randomForest()
modelo_rf <- randomForest(x)
modelo_rf <- randomForest(x, y=NULL)
